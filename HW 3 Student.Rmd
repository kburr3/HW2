---
title: "HW 3"
author: "Keegan Burr"
date: "10/7/2024"
output:
  pdf_document: default
  html_document:
    number_sections: yes
---

#

Let $E[X] = \mu$.  Show that $Var[X] := E[(X-E[X])^2] = E[X^2]-(E[X])^2$.  Note, all you have to do is show the second equality (the first is our definition from class). 
### My answer
# First expand the squared term
$X^2-2XE[X]+(E[X])^2$
# Add E outside all of it
$E[X^2-2XE[X]+(E[X])^2]$
$E[X^2]-2E[XE[X]]+E[(E[X])^2]$
# Simplify (Note: E[X] is a constant)
$E[X^2]-2(E[X])^2+(E[X])^2$
$E[X^2]-(E[X])^2$
# So finally we would have
$Var[X] := E[(X-E[X])^2] = E[X^2]-(E[X])^2$


# 

In the computational section of this homework, we will discuss support vector machines and tree-based methods.  I will begin by simulating some data for you to use with SVM. 

```{r}
library(e1071)
set.seed(1) 
x=matrix(rnorm(200*2),ncol=2)
x[1:100,]=x[1:100,]+2
x[101:150,]=x[101:150,]-2
y=c(rep(1,150),rep(2,50))
dat=data.frame(x=x,y=as.factor(y))
plot(x, col=y)

```


##

Quite clearly, the above data is not linearly separable.  Create a training-testing partition with 100 random observations in the training partition.  Fit an svm on this training data using the radial kernel, and tuning parameters $\gamma=1$, cost $=1$.  Plot the svm on the training data.  

```{r}
set.seed(1)
train <- sample(200,100)
svmfit <- svm(y ~ ., data = dat[train,], kernel = "radial", gamma = 1, cost = 1)
print(svmfit)

plot(svmfit, dat[train,])
```

##

Notice that the above decision boundary is decidedly non-linear.  It seems to perform reasonably well, but there are indeed some misclassifications.  Let's see if increasing the cost ^[Remember this is a parameter that decides how smooth your decision boundary should be] helps our classification error rate.  Refit the svm with the radial kernel, $\gamma=1$, and a cost of 10000.  Plot this svm on the training data. 

```{r}
svmfit <- svm(y ~ ., data = dat[train,], kernel = "radial", gamma = 1, cost = 10000)
plot(svmfit, dat[train,])
```

##

It would appear that we are better capturing the training data, but comment on the dangers (if any exist), of such a model. 

*If we were to implement a high cost, it forces the model to attempt to correctly classify almost every training data point leading us to an overfit model that would capture small flucations in the data that maybe shouldn't. It is likely better to weaken the linearity constraint because there is a clear non-linear pattern, but weaken this constraint will lead to a trade-off particularly pertaining to overfitting as this would generalize the model to future data. The model may fit well to the current training data, but it's performance will likely decrease when it is implemented on new data.*

##

Create a confusion matrix by using this svm to predict on the current testing partition.  Comment on the confusion matrix.  Is there any disparity in our classification results?    

```{r}
#remove eval = FALSE in above
table(true = dat[-train,"y"], pred = predict(svmfit, newdata = dat[-train,]))
```
# In our confusion matrix above overall the model performs pretty well in classifying instances of 1, with 67 instances correct and only 12 misclassifcations. Similarly with instances of 2 the model correctly predicts 19 instances correct and only had 2 misclassifcations. I do worry slightly about the disparity in the classification rates as instance 1 has misclassification rate of ~15% and instance 2 of ~9%. Although not a huge difference, this misclassifcation could suggest the model is overfitting to the training data.


##

Is this disparity because of imbalance in the training/testing partition?  Find the proportion of class `2` in your training partition and see if it is broadly representative of the underlying 25\% of class 2 in the data as a whole.  

```{r}
# View table 
dat[train,]
# Determine in the 'y' column how many have class 2
sum(dat[train, "y"] == 2)


```

*29/100 is obviously 29%, which is pretty close to the underlying percentage. This leads me to believe that the disparity is likley not because of an imbalance between the training/testing data but likely because of overfitting.*

##

Let's try and balance the above to solutions via cross-validation.  Using the `tune` function, pass in the training data, and a list of the following cost and $\gamma$ values: {0.1, 1, 10, 100, 1000} and {0.5, 1,2,3,4}.  Save the output of this function in a variable called `tune.out`.  

```{r}
set.seed(1)
tune.out <- tune(svm, y~., data = dat[train,], kernel = "radial", ranges = list(cost = c(0.1, 1, 10, 100, 1000), gamma = c(0.5, 1,2,3,4)))
```

I will take `tune.out` and use the best model according to error rate to test on our data.  I will report a confusion matrix corresponding to the 100 predictions.  


```{r}
table(true = dat[-train,"y"], pred = predict(tune.out$best.model, newdata = dat[-train,]))
```

##

Comment on the confusion matrix.  How have we improved upon the model in question 2 and what qualifications are still necessary for this improved model.  

*So now it seems like we are not overfitting as badly and our model preforms a little better. Our misclassifcations for both instances 1 and 2 reduced (for 1 it reduced from 12 to 7 and for 2 it reduced from 2 to 1), but still with our data imbalance, instances 1 are still misclassified more often than instances of 2.* 

# 
Let's turn now to decision trees.  

```{r}

library(kmed)
data(heart)
library(tree)

```

## 

The response variable is currently a categorical variable with four levels.  Convert heart disease into binary categorical variable.  Then, ensure that it is properly stored as a factor. 

```{r}
for (i in 1:nrow(heart)) {
  if (heart$class[i] > 0) {
    heart$class[i] <- 1}
}
heart$class <- as.factor(heart$class)
```

## 

Train a classification tree on a 240 observation training subset (using the seed I have set for you).  Plot the tree.  

```{r}
set.seed(101)

train_heart <- sample(1:nrow(heart), 240)
tree.heart <- tree(class~., heart, subset=train_heart)
plot(tree.heart)
text(tree.heart, pretty=0)
```


## 

Use the trained model to classify the remaining testing points.  Create a confusion matrix to evaluate performance.  Report the classification error rate.  

```{r}
tree.pred <- predict(tree.heart, heart[-train_heart,], type="class")
with(heart[-train_heart,], table(tree.pred, class))

# So then using results
# 1-(28+18)/57 = ~.193
```

##  

Above we have a fully grown (bushy) tree.  Now, cross validate it using the `cv.tree` command.  Specify cross validation to be done according to the misclassification rate.  Choose an ideal number of splits, and plot this tree.  Finally, use this pruned tree to test on the testing set.  Report a confusion matrix and the misclassification rate.  

```{r}
set.seed(101)

cv.heart = cv.tree(tree.heart, FUN = prune.misclass)
cv.heart
plot(cv.heart$size, cv.heart$dev, type = "b")

# Take some fully grown bushy tree and prune it to the user specified number of splits (in this case 3 looks best)
prune.heart = prune.misclass(tree.heart, best = 3)
plot(prune.heart)
text(prune.heart, pretty=0)

tree.pred = predict(prune.heart, heart[-train_heart,], type="class")
with(heart[-train_heart,], table(tree.pred, class))
# So then using results
# 1-((26+17)/57) = ~.246
```


##

Discuss the trade-off in accuracy and interpretability in pruning the above tree. 

*By pruning the tree we are basically "collasping" branches together to make a less complex tree, but by doing this we often times give up some accuracy (in this case it was about 5%) in the classification rate for easier interpretability of the tree.*

## 

Discuss the ways a decision tree could manifest algorithmic bias.  

*A couple ways a decision tree could manifest algorithic bias are bias in the training/testing data and class imbalance (as we saw earlier in the homework). This could lead us to overfitting to biased data which would lead to poor/inaccurate results*